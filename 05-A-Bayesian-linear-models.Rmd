---
layout: topic
title: "Bayesian Inference from Linear Models"
author: Jes, Jamie
output: html_document
---

**Assigned Reading:**

> Chapters 3, 4.1, 8.4, 9.2, in Korner-Nievergelt et al. 2015. *Bayesian data analysis in ecology using linear models with R, BUGS, and Stan.* Elsevier. [link](http://www.sciencedirect.com/science/book/9780128013700)
>
> \* These chapters use the same functions you have already seen. Read the assigned sections focusing on how we make Bayesian inferences from models fit with these functions.

**Optional Reading:**

> Chapters 5, 4.2, 8, 9, in Korner-Nievergelt et al. 2015. *Bayesian data analysis in ecology using linear models with R, BUGS, and Stan.* Elsevier. [link](http://www.sciencedirect.com/science/book/9780128013700) 
>
> \* Chapter 4.2 focuses on ANOVA-type models while the rest of chapters 8 and 9 focus on Binomial and Binary models.

```{r include = FALSE}
# This code block sets up the r session when the page is rendered to html
# include = FALSE means that it will not be included in the html document

# Write every code block to the html document 
knitr::opts_chunk$set(echo = TRUE)

# Write the results of every code block to the html document 
knitr::opts_chunk$set(eval = TRUE)

# Define the directory where images generated by knit will be saved
knitr::opts_chunk$set(fig.path = "images/05-A/")

# Set the web address where R will look for files from this repository
# Do not change this address
repo_url <- "https://raw.githubusercontent.com/fukamilab/BIO202/master/"
```

### Key Points

Differences between Frequentist and Bayesian approach to statistical inference:

**See Table 3.1 in the reading.**

| Bayesian         | Frequentist       |
|------------------|-------------------|
| Make probabalistic statements about our state of knowledge. | Ask how parameter estimates change if data collected many times. |
| Calculate probability of hypothesis, given observed data.| Calculate probability of data, given null hypothesis. |
| Likelihood is a function that calculates probability for any set of data and parameters. | Likelihood is a value- the probability of the observed set of data, given a specific model and estimated parameters.|
| Inference based on samples of parameters from the posterior distribution. | Inference based on null hypothesis tests on values of estimated parameters and their SE.|

**Bayesian approach to statistical inference:**

If  $\theta$ is a model and $y$ is observed data, then frequentists estimate $P(y|\theta)$ and Bayesians estimate $P(\theta|y)$. Bayesians use:

$$p(\theta|y) = \frac{p(y|\theta) p(\theta)}{p(y)}$$

$p(\theta|y)$ is the posterior distibution,  $p(\theta)$ is our prior information about what values the parameters can take, and $p(y|\theta)$ is the likelihood function that gives the probability of an observed data point given a set of parameter values and a model.


+ Development of algorithms that sample from posterior $p(\theta|y)$ without knowing the prior probability of the data $p(y) = \int p(y|\theta)p(\theta) d\theta$ have made Bayesian inference accessible.

+ Types of posterior distributions:
    + Joint distribution: gives probability of a set of parameter values. (Plot parameter samples to see whether parameters are correlated!)
    + Marginal distribution: gives probabilty for a particular parameter summed over all values of other parameters. (This is what you get if you summarize parameter samples independently.)
    + Conditional distribution: gives probabilty for a particular parameter holding other parameters at specific values.

Joint posterior distribution of $\beta_0$ and $\beta_1$:
```{r echo = FALSE, warning = FALSE}
library(ggplot2)

# Set random seed so figures aren't changed every time
set.seed(13)

# Make fake random sample of parameters:
beta1 <- rnorm(5000, 6, 2)
beta0 <- 6 - beta1 + rnorm(5000, 0, 2)
beta <- data.frame(beta0, beta1)

# Plot joint posterior
ggplot(beta, aes(x = beta0, y = beta1)) + 
  geom_point(alpha = 0.5) + 
  geom_hline(yintercept = 4, color = 'red') +
  geom_hline(yintercept = 8, color = 'blue') +
  labs(x = expression(beta[0]), 
       y = expression(beta[1])
  )

```

Marginal distribution of $\beta_0$:
```{r echo = FALSE, warning = FALSE}
ggplot(beta, aes(x = beta0)) +
  geom_freqpoly(binwidth = 1) + 
  labs(x = expression(beta[0]),
       y = "Num. samples")

```

Conditional distribution of $\beta_0$ when $\beta_1 = 4$ (red line) or $\beta_1 = 8$ (blue line).

```{r echo = FALSE, warning = FALSE}
library(tidyr)

beta0_4 <- 6 - 4 + rnorm(1000, 0, 2)
beta0_8 <- 6 - 8 + rnorm(1000, 0, 2)
beta_cond <- data.frame(beta0_4, beta0_8)

beta_long <- beta_cond %>%
  gather(key = "parm_cond", value = "value")

ggplot(beta_long, aes(x = value, color = parm_cond)) +
  geom_freqpoly(binwidth = 1) + 
  scale_color_manual(values = c("red", "blue"), labels = expression(beta[1]==4, beta[1]==8)) + 
  labs(x = expression(beta[0]),
       y = "Num. samples",
       color = NULL)

```


+ Inference is based on **visualizing** and summarizing the posterior distributions of parameters, predicted future observations, and quantities calculated from parameters.
    
+ Make inferences (e.g. about effects of covariates) by summarizing samples of parameters from the posterior distribution:
    + Calculate median of samples to see central tendency of marginal distribution of a parameter.
    + Visualize uncertainty with credibility intervals (CrI). These give two values of a parameter between which a given percentage of samples occur. 
        + Can be based on quantiles (symmetric), e.g. 90% CrI is between the 0.05 and 0.95 quantiles of the samples of a parameter (use `quantile()`)
        + Can be the interval with the highest posterior density (HPD): probability density within interval is never lower than probability density outside interval (use `HPDinterval(as.mcmc())`, both functions from `coda` package).
    + Calculate $\hat{y}$ from samples of parameters to estimate uncertainty around predicted mean.
    
+ Make inferences about uncertainty in future observations by sampling from the posterior predictive distribution.
    + Use calculated samples of $\hat{y}$ together with the likelihood (and potentially samples of $\hat{\sigma}$) to predict future observations.
    + Combines uncertainty in parameter estimation with uncertainty in the probability model.
+ Can easily calculate probabilities associated any hypothesis derived from model parameters:
    + Probability that future observations will be between two values.
    + Probability that the variance of $y$ among sites is greater than the variance of $y$ within sites.
    + Other examples?
    
+ Bayesians use log pointwise predictive density to estimate model fit, which is analogous to the log-likelihood in Frequentist inference (see Chapter 5 and 11.2 in Korner-Nievergelt et al. 2015). For each observed outcome, it calculates the average probability of observing that outcome across all of the samples from the posterior, then adds together the ln of these probabilities.

+ Before drawing conclusions, make sure to assess the model assumptions and validate the model by looking at the residual deviance and plotting residuals.

**Model notation**

To use Bayesian methods you should learn how to write down your model using matrix notation. Practice doing this with every model you fit and identifying what the columns and rows are in each matrix. Doing so will make it easier to write your own models in BUGS or Stan, which will be necessary for fitting more complex models. 

$$\mathbf{y} \sim Pois(\boldsymbol{\hat{\lambda}})\\
log(\boldsymbol{\hat{\lambda}}) = \mathbf{X}\boldsymbol{\hat{\beta}}$$

$\mathbf{y}$ is a vector of observations (of counts) with length $n$.

$\boldsymbol{\hat{\lambda}}$ is a vector of the estimated mean for each observation (has length $n$).

$\mathbf{X}$ is a matrix with $n$ rows and $p$ columns with the observed values of the $p - 1$ covariates in each of the rows. (Note that we have an initial column of all $1$ to estimate the mean.)

$\boldsymbol{\hat{\beta}}$ is a vector of the parameters to be estimated with length $p$.


**Bayesian inference in R**

+ Use the `sim()` function from `arm` package to simulate samples from the posterior for models fit with `lm()`, `glm()`, `glmer()`.
+ `sim()` assumes uniform (flat) prior distributions on all parameters, calculates the posterior distribution analytically, then generates random samples from this distribution.
+ Table 8.1 gives good summary of possible GLM likelihood functions and their common link functions.
+ `model.matrix()` makes it easy to create design matrices for calculating fitted values ($\hat{y}$) from samples of parameters.
+ Use the `apply()` function to summarize the matrix of parameter samples returned by `sim(model, n.sim)@coef`. `apply(matrix, 1, fun)` applies a function `fun` to the rows of the matrix whereas `apply(matrix, 2, fun)` applies the function to the columns.
+ Use `%*%` to do matrix multiplication to calculate $\hat{y}$ from samples of parameters and values of covariates.





### Analysis Example
Let's use the data from Appendix A of the Zuur book, where we are looking at the density of birds within 56 forest patches. ABUND= the density of birds in a forest patch (continuous response variable), ALT= mean altitude of the patch (continuous explanatory variable). We want to know how mean altitude of the forest patch influences the number of birds within the patch. This code comes from Chapter 3 and Chapter 4.1 in Korner-Nievergelt et al. 2015. *Bayesian data analysis in ecology using linear models with R, BUGS, and Stan.

#### Step 1: Fit a linear model  
Remember you could use any linear model from lm() glm() or glmer() but we are going to use a lm() example.  
```{r include = TRUE}
# clear your environment
rm(list=ls()) 

# read in the data
#Loyn<- read.table(file = "/Users/jamiemcdevitt-irwin/Dropbox/Jamie/Stanford/3_Courses/Biol202/Week3_linearmodels_intro/ZuurDataMixedModelling/Loyn.txt", header = TRUE, dec = ".")   

Loyn <- read.table("data/Loyn.txt", header = TRUE)

# let's take a quick look at the data
head(Loyn)

# run a linear regression with just one of the explanatory variables: ALT
M1 <- lm(ABUND ~ ALT, data = Loyn) # least squares fit 
summary(M1)
summary(M1)$sigma # pull out the residual standard error 

```
To get an idea of how sim() draws upon our model, let's first write out this model fully.  
**Random**   
$ABUND_i \sim N(\hat{ABUND_i}, \hat{sigma})$    
**Deterministic**    
$\hat{ABUND_i} = \hat{beta_0} + \hat{beta_1} * ALT_i$  


#### Step 2: Draw Conclusions
Now lets use sim() in the package 'arm' on the model we produced 'M1'. Sim() draws a random value from the marginal posterior distribution of sigma and then draws random values from the conditional posterior distribution of $\beta$ (i.e. the parameters in the model, in our case the intercept and slope for ALT). These posterior distributions describe the range of plausible parameter values given the data and the model. They express our uncertainty about the model parameters. 


##### Simulate values for theta and sigma 
```{r include = TRUE, warning = FALSE}
#install.packages('arm')
library(arm)
nsim <-1000 # number of simulations can be changed to whatever you want
bsim <- sim(M1, n.sim=nsim)
str(bsim)
```

sim() produces an object of class sim that contains "coef" (1000 simulated values for theta, i.e. your parameters) and "sigma" containing the 1000 values for SD


##### Summary statistics to describe posterior knowledge about the parameters (theta) and the variance
```{r include = TRUE}
apply(coef(bsim), 2, quantile, prob=c(0.025, 0.975)) # parameters

quantile(bsim@sigma, prob=c(0.025, 0.975)) # sigma
```


##### What if we want to test hypotheses about our data using the posterior distribution?    
**H1.** 



**H2.** How sure are we that the slope parameter for ALT is larger than .05, .1, .2?
```{r include = TRUE}
set.seed(100) # before you run the sim function
sum(coef(bsim)[,2]>.05)/nsim # 93% confident that the slope is greater than .05
sum(coef(bsim)[,2]>.1)/nsim # 44% confident that the slope is greater than .1
sum(coef(bsim)[,2]>.2)/nsim # 0.1% confident that the slope is greater than .2

```

##### What if we want to look at the effect of x (ALT) on y (ABUND) graphically and include information about the uncertainty of the parameter estimates?

We use simulated values from the posterior distribution of model parameters. The simulation of the posterior distribution gives 1000 pairs of intercepts and slopes that describe 1000 different regression lines. We then draw these lines to show the uncertainty in our regression line estimation

```{r include = TRUE}

plot(Loyn$ALT, Loyn$ABUND, pch=16, las=1, cex.lab=1.2) 
for(i in 1:nsim) abline(coef(bsim)[i,1], coef(bsim)[i,2], col=rgb(0,0,0,0.05)) # add semitransparent regression lines


```

So the above plot shows you the 1000 possible regression lines that were simulated from the sim() function. The points on the plot are the original values of ABUND vs ALT  

##### Plot the 95% CrI's
95% CrI's: credible interval within which we expect the true parameter value to be with a probability of 0.95  
```{r include = TRUE}
# first we must define new x-values for which we could like to make fitted values
# find the max and min of ALT
min(Loyn$ALT) # 60
max(Loyn$ALT) # 260
# now save these x-values because we want to then predict values past these 
newdat <- data.frame(ALT=seq(60, 260, by=0.1))
head(newdat)
newmodmat <- model.matrix(~ALT, data=newdat) # create a matrix that contains these new x-values
head(newmodmat)
dim(newmodmat)
fitmat <- matrix(ncol=nsim, nrow=nrow(newdat)) # then calculate the 1000 fitted values for each new x value using matrix multiplication
head(fitmat)
dim(fitmat)

for(i in 1:nsim) fitmat[,i] <- newmodmat %*% coef(bsim)[i,] # %*% is for matrix multiplication, recall that bsim is the object sim() produced for our model M1

# so you are multiplying the new x values created from the min and max of x by the bsim values  
plot(Loyn$ALT,Loyn$ABUND, pch=16, las=1, cex.lab=1.2)
abline(M1, lw=2) # add line from our original model
lines(newdat$ALT, apply(fitmat, 1, quantile, prob=0.025), lty=3)
lines(newdat$ALT, apply(fitmat, 1, quantile, prob=0.975), lty=3)  

```
  
We are 95% sure the true regression line (black line) is within the credible interval (dotted lines)



##### What if we want to predict future observations?
```{r include = TRUE}
# prepare matrix for simulated new data
newy <- matrix(ncol=nsim, nrow=nrow(newdat)) # recall newdata is the new x values we created earlier  

# for each simulated fitted value (x=ALT), simulate one new y-value (y=ABUND)
for(i in 1:nsim) newy[,i] <- rnorm(nrow(newdat),mean=fitmat[,i],sd=bsim@sigma[i]) 

# call the plot with the 95% credible interval prior to adding the 95% interval of simulated predictive distribution
plot(Loyn$ALT,Loyn$ABUND, pch=16, las=1, cex.lab=1.2)
abline(M1, lw=2) # add line from our original model
lines(newdat$ALT, apply(fitmat, 1, quantile, prob=0.025), lty=3)
lines(newdat$ALT, apply(fitmat, 1, quantile, prob=0.975), lty=3)

# 95% interval of simulated predictive distribution
lines(newdat$ALT, apply(newy, 1, quantile, prob=0.025), lty=2) 
lines(newdat$ALT, apply(newy, 1, quantile, prob=0.975), lty=2)


```

Thus, future observations are expected to be within the interval defined by the broken lines in the figure above with a probability of 0.95.

Note: compared to a frequentist approach (predict()), this takes a lot more code. However, when you have a simulated the posterior predictive distribution, you have more information than is contained in the frequentist prediction interval. For example, you can test hypotheses about future observations (see code below)


##### From these predicted future values, we can give an estimate for the proportion of observations (i.e. density of birds) greater than #, given the mean altitude = #
**EX1.** If ALT=60, what is the estimated proportion of observations greater than 20?
```{r include = TRUE}
sum(newy[newdat$ALT==60,]>20)/nsim
```
Thus we expect 20% of future observations with ALT=60 to have a density of birds greater than 20


**EX2.** IF ALT=120, what is the estimated proportion of observations greater than 20?
```{r include = TRUE}
sum(newy[newdat$ALT==120,]>20)/nsim

```
Thus we expect 39% of future observations with ALT=120 to have a density of birds greater than 20


Another reason to learn this is because once you have a more complicated model (e.g. mixed models), the frequentist methods to obtain confidence intervals of fitted values are much more complicated than the Bayesian method presented here. The code we learned here can be used for mixed models and generalized linear mixed models with minor changes.   

### Discussion Questions
1) Why is it harder to obtain CI for fitted values in frequentist approaches?  
2) When/Why would you bother to use sim() if you don't have priors? Would it help validate your lm()?   
3) What type of prior knowledge could you think of adding to this example data (X=Altitude, Y=Density) (if it existed)?    
4) Where would the priors go into the model equation (i.e. write out the model equation in matrices, where would the priors be?)?  





